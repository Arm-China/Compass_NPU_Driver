// Copyright (C) 2023-2025 Arm Technology (China) Co. Ltd.
//
// SPDX-License-Identifier: Apache-2.0

/**
 * @file  export_py_api.cpp
 * @brief AIPU User Mode Driver (UMD) C++ to Python API implementation
 * @version 1.0
 */

#include <errno.h>
#include <fcntl.h>
#include <pybind11/functional.h>
#include <sys/mman.h>
#include <unistd.h>

#include <cstring>
#include <map>
#include <string>
#include <tuple>
#include <vector>

#include "pybind11/numpy.h"
#include "pybind11/pybind11.h"
#include "pybind11/stl.h"
#include "standard_api.h"

#define PATH_LEN (1024u)

namespace py = pybind11;

namespace aipudrv {
struct aipu_job_config_dump_wrapper_t {
  /* dump_dir is used as file dump-path */
  std::string dump_dir;
  /* name prefix of dump files */
  std::string prefix;
  /* name prefix of output dump files */
  std::string output_prefix;
  /* name prefix of profile/printf data files */
  std::string misc_prefix;
};

struct aipu_job_config_simulation_wrapper_t {
  /* data_dir is used as aipu v1/v2 simulation data file directory */
  std::string data_dir;
};

struct aipu_load_graph_cfg_wrapper_t {
  uint8_t wt_mem_region = 0;     /* [[deprecated]] */
  std::vector<int32_t> wt_idxes; /* [[deprecated]] */
  /* specify path of extra weight, which is generated by model compile */
  std::string extra_weight_path;
  /* put whole weight to NPU global memory, only for small model accelerating */
  bool put_weight_gm = false;
  /* put whole descriptor to NPU global memory, only for small model
   * accelerating */
  bool put_desc_gm = false;
  /* put whole workspace to NPU global memory, only for small model accelerating
   */
  bool put_ws_gm = false;
};

struct aipu_global_config_simulation_wrapper_t {
  std::string simulator;
  std::string log_file_path;
  std::string npu_arch_desc; /* [[deprecated]]: ignore */
  std::string plugin_name;
  std::string json_filename;
  uint32_t log_level;
  uint32_t gm_size;
  bool verbose;
  bool enable_avx = 0;
  bool enable_calloc = 0;
  bool en_eval = 0;
  bool en_l2d = 0;

  /**
   * fast evaluation config for aipu v3_2
   */
  bool en_fast_perf;
  uint32_t freq_mhz = 1000;
  uint32_t ddr_latency_rd = 0;
  uint32_t ddr_latency_wr = 0;
  uint32_t ddr_bw = 256;
  float ddr_bw_ratio = 1.0;
  std::string perf_report = "./perf.csv";
};

/**
 * @struct aipu_dynshape_rank_t
 *
 * @brief get rank of specified input, and mainly for
 * `aipu_ioctl/AIPU_IOCTL_GET_DS_RANK`
 *
 * @note e.g.[N,H,W,C] rank is 4, and 'ds_idx' is the index of inputs which
 * includes static and dynamic input
 */
struct aipu_dynshape_rank_wrapper_t {
  /* the graph id to be inquired */
  uint64_t graph_id;
  /* the dynamic shape index */
  uint32_t ds_idx;
  /* [out] the rank of 'ds_idx'th input */
  uint32_t rank;
};

/**
 * @struct aipu_dynshape_dimension_wrapper_t
 *
 * @brief get min/max dimension constraint of specified input, and mainly for
 * `aipu_ioctl/AIPU_IOCTL_GET_DS_DIM_CONSTRAINT`. rank information should get
 * from `aipu_ioctl/AIPU_IOCTL_GET_DS_RANK`
 */
struct aipu_dynshape_dimension_wrapper_t {
  /* the graph id to be searched */
  uint64_t graph_id;
  /* the dynamic shape index */
  uint32_t ds_idx;
  /* [out] the minium dimensions of 'ds_idx'th input */
  std::vector<uint32_t> min_dim;
  /* [out] the maximum dimensions of 'ds_idx'th input */
  std::vector<uint32_t> max_dim;
};

struct aipu_dynshape_param_wrapper_t {
  /* outer is input index, and inner is each dimension of each input, such as
   * [[1,128], [1,224,224,3]]*/
  std::vector<std::vector<uint32_t>> shape_items;
};

struct aipu_create_job_cfg_wrapper_t {
  /* all targets have only 1 partition, so it is 0 */
  uint8_t partition_id = 0;
  /* debug dispatch flag, set 1 to indicate specify job to debug core to run */
  uint8_t dbg_dispatch = 0;
  /* specify debug core id, [0, max_core_id in cluster] */
  uint8_t dbg_core_id = 0;
  /* defalut 0, low priority, only for aipu v3 */
  uint8_t qos_level = 0;
  /* default 0, feature map buffer memory region */
  uint8_t fm_mem_region = 0;
  /* specify feature maps indexes allocated from 'fm_mem_region' */
  std::vector<int32_t> fm_idxes;
  /* dynamic shape parameter, you can also set or update it by 'aipu_config_job'
   */
  aipu_dynshape_param_wrapper_t dynshape;
};

namespace {
aipu_load_graph_cfg_t parse_graph_cfg(const py::handle &cfg,
                                      std::vector<int32_t> &idxes,
                                      std::string &extra_weight_path) {
  aipu_load_graph_cfg_t cfg_ = {0};
  if (py::isinstance<aipu_load_graph_cfg_wrapper_t>(cfg)) {
    aipu_load_graph_cfg_wrapper_t wrapper =
        py::cast<aipu_load_graph_cfg_wrapper_t>(cfg);
    cfg_.wt_mem_region = wrapper.wt_mem_region;
    if (wrapper.wt_idxes.size() != 0) {
      idxes = wrapper.wt_idxes;
      cfg_.wt_idxes_cnt = idxes.size();
      cfg_.wt_idxes = idxes.data();
    }

    if (!wrapper.extra_weight_path.empty()) {
      extra_weight_path = wrapper.extra_weight_path;
      cfg_.extra_weight_path = extra_weight_path.c_str();
    }
  } else if (py::isinstance<py::dict>(cfg)) {
    py::dict cfg_d = py::cast<py::dict>(cfg);
    for (auto item : cfg_d) {
      std::string key = py::cast<std::string>(item.first);
      if (key == "wt_mem_region") {
        cfg_.wt_mem_region = py::cast<uint8_t>(item.second);
      } else if (key == "wt_idxes") {
        idxes = py::cast<std::vector<int32_t>>(item.second);
        cfg_.wt_idxes_cnt = idxes.size();
        cfg_.wt_idxes = idxes.data();
      } else if (key == "extra_weight_path") {
        extra_weight_path = py::cast<std::string>(item.second);
        cfg_.extra_weight_path = extra_weight_path.c_str();
      } else if (key == "put_weight_gm") {
        cfg_.put_weight_gm = py::cast<bool>(item.second);
      } else if (key == "put_desc_gm") {
        cfg_.put_desc_gm = py::cast<bool>(item.second);
      } else if (key == "put_ws_gm") {
        cfg_.put_ws_gm = py::cast<bool>(item.second);
      }
    }
  } else if (!cfg.is_none()) {
    throw std::runtime_error("[PY UMD ERROR] graph cfg is invalid");
  }

  return cfg_;
}
} // namespace

class NPU {
public:
  /**
   * @brief This API is used to initialize AIPU UMD context
   *
   *
   * @retval AIPU_STATUS_SUCCESS if success
   *
   * @note Before invoking any other UMD API calls, any UMD application must
   * initialize a context first.
   */
  aipu_status_t aipu_init_context_py() { return aipu_init_context(&m_ctx); }

  /**
   * @brief This API is used to destroy AIPU UMD context
   *
   * @retval AIPU_STATUS_SUCCESS if success
   */
  aipu_status_t aipu_deinit_context_py() { return aipu_deinit_context(m_ctx); }

  /**
   * @brief This API is used to track what happens when an error code is
   * returned by a UMD API.
   *
   * @param[in]  status Status returned by UMD standard API
   *
   * @retval error mesage
   */
  std::string aipu_get_error_message_py(uint64_t status) {
    const char *status_msg = nullptr;

    aipu_get_error_message(m_ctx, (aipu_status_t)status,
                           (const char **)&status_msg);
    return std::string(status_msg);
  }

  /**
   * @brief This API is used to configure a specified global option.
   * @param[in] types:
   *  - AIPU_CONFIG_TYPE_HW
   *  - AIPU_CONFIG_TYPE_SIMULATION
   *  - AIPU_GLOBAL_CONFIG_TYPE_DISABLE_VER_CHECK
   *  - AIPU_GLOBAL_CONFIG_TYPE_ENABLE_VER_CHECK
   * @param[in] cfg:
   *  - `types`: AIPU_CONFIG_TYPE_HW, `cfg`: aipu_global_config_hw_t
   *  - `types`: AIPU_CONFIG_TYPE_SIMULATION, `cfg`:
   * aipu_global_config_simulation_wrapper_t
   *  - `types`: AIPU_GLOBAL_CONFIG_TYPE_DISABLE_VER_CHECK, `cfg`: none
   *  - `types`: AIPU_GLOBAL_CONFIG_TYPE_ENABLE_VER_CHECK, `cfg`: none
   *
   * @retval AIPU_STATUS_SUCCESS if success
   *
   * @note accepted types/config: AIPU_CONFIG_TYPE_HW/`aipu_global_config_hw_t`
   * @note accepted types/config:
   * AIPU_CONFIG_TYPE_SIMULATION/`aipu_global_config_simulation_wrapper_t`
   * @note accepted types/config: AIPU_GLOBAL_CONFIG_TYPE_DISABLE_VER_CHECK/none
   * @note accepted types/config: AIPU_GLOBAL_CONFIG_TYPE_ENABLE_VER_CHECK/none
   */
  aipu_status_t aipu_config_global_py(uint64_t types, const py::handle &cfg) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    const char *status_msg = nullptr;

    if (types & AIPU_CONFIG_TYPE_HW) {
      aipu_global_config_hw_t cfg_ = py::cast<aipu_global_config_hw_t>(cfg);
      ret = aipu_config_global(m_ctx, AIPU_CONFIG_TYPE_HW, &cfg_);
    } else if (types & AIPU_CONFIG_TYPE_SIMULATION) {
      aipu_global_config_simulation_wrapper_t wrapper =
          py::cast<aipu_global_config_simulation_wrapper_t>(cfg);
      aipu_global_config_simulation_t cfg_;
      memset(&cfg_, 0, sizeof(aipu_global_config_simulation_t));
      if (!wrapper.simulator.empty())
        cfg_.simulator = wrapper.simulator.c_str();
      if (!wrapper.log_file_path.empty())
        cfg_.log_file_path = wrapper.log_file_path.c_str();
      if (!wrapper.npu_arch_desc.empty())
        cfg_.npu_arch_desc = wrapper.npu_arch_desc.c_str();
      if (!wrapper.plugin_name.empty())
        cfg_.plugin_name = wrapper.plugin_name.c_str();
      if (!wrapper.json_filename.empty())
        cfg_.json_filename = wrapper.json_filename.c_str();
      if (!wrapper.perf_report.empty())
        cfg_.perf_report = wrapper.perf_report.c_str();

      cfg_.log_level = wrapper.log_level;
      cfg_.gm_size = wrapper.gm_size;
      cfg_.verbose = wrapper.verbose;
      cfg_.enable_avx = wrapper.enable_avx;
      cfg_.enable_calloc = wrapper.enable_calloc;
      cfg_.en_eval = wrapper.en_eval;
      cfg_.en_l2d = wrapper.en_l2d;
      cfg_.en_fast_perf = wrapper.en_fast_perf;
      cfg_.freq_mhz = wrapper.freq_mhz;
      cfg_.ddr_latency_rd = wrapper.ddr_latency_rd;
      cfg_.ddr_bw = wrapper.ddr_bw;
      cfg_.ddr_bw_ratio = wrapper.ddr_bw_ratio;
      ret = aipu_config_global(m_ctx, AIPU_CONFIG_TYPE_SIMULATION, &cfg_);
    } else {
      ret = aipu_config_global(m_ctx, types, nullptr);
    }

    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_config_global: %s\n", status_msg);
    }
    return ret;
  }

  /**
   * @brief This API loads an offline built AIPU executable graph binary from
   * file system.
   *
   * @param[in]  graph_bin Executable graph binary file path
   * @param[in]  cfg  Configuration in loading graph stage with
   * `aipu_load_graph_cfg_wrapper_t` or dict dict key support:
   *                      - 'wt_mem_region'
   *                      - 'wt_idxes'
   *                      - 'extra_weight_path'
   *                      - 'put_weight_gm'
   *                      - 'put_desc_gm'
   *                      - 'put_ws_gm'
   * @retval     tuple:
   *              - aipu_status_t: AIPU_STATUS_SUCCESS if success
   *              - graph_id
   */
  py::tuple aipu_load_graph_py(const std::string &graph_bin,
                               const py::handle &cfg) {
    std::vector<int32_t> idxes;
    std::string path;
    aipu_load_graph_cfg_t cfg_ = parse_graph_cfg(cfg, idxes, path);

    uint64_t graph_id = 0;
    aipu_status_t ret = aipu_load_graph(m_ctx, graph_bin.c_str(), &graph_id,
                                        cfg.is_none() ? nullptr : &cfg_);
    if (ret != AIPU_STATUS_SUCCESS) {
      const char *status_msg = nullptr;
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_load_graph: %s\n", status_msg);
      return py::make_tuple(ret, 0);
    }
    return py::make_tuple(ret, graph_id);
  }

  /**
   * @brief This API loads a graph with the form of AIPU executable graph
   * buffer.
   *
   * @param[in]  graph_buffer The start address of buffer which stores graph
   * binary data
   * @param[in]  graph_size The byte size of graph binary data in 'graph_buffer'
   * @param[in]  cfg  Configuration in loading graph stage with
   * `aipu_load_graph_cfg_wrapper_t` or dict dict key:
   *                      - 'wt_mem_region'
   *                      - 'wt_idxes'
   *                      - 'extra_weight_path'
   *                      - 'put_weight_gm'
   *                      - 'put_desc_gm'
   *                      - 'put_ws_gm'
   * @retval     tuple
   *              - aipu_status_t: AIPU_STATUS_SUCCESS if success
   *              - grpah_id
   */
  py::tuple aipu_load_graph_helper_py(py::handle graph_buffer,
                                      uint32_t graph_size,
                                      const py::handle &cfg) {
    const char *data = nullptr;
    if (py::isinstance<py::bytes>(graph_buffer)) {
      /* only refernece, no copy */
      py::bytes by = py::cast<py::bytes>(graph_buffer);
      data = PyBytes_AsString(by.ptr());
    } else if (py::isinstance<py::array>(graph_buffer)) {
      /* DONOT need to consider non-c-continous */
      py::array arr = py::cast<py::array>(graph_buffer);
      data = reinterpret_cast<const char *>(arr.request().ptr);
    } else {
      fprintf(
          stderr,
          "[PY UMD ERROR] only accept numpy.ndarray, bytes or array.array\n");
      return py::make_tuple(AIPU_STATUS_ERROR_UNKNOWN_BIN, 0);
    }

    std::vector<int32_t> idxes;
    std::string path;
    aipu_load_graph_cfg_t cfg_ = parse_graph_cfg(cfg, idxes, path);

    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    uint64_t graph_id = 0;
    ret = aipu_load_graph_helper(m_ctx, data, graph_size, &graph_id,
                                 cfg.is_none() ? nullptr : &cfg_);

    if (ret != AIPU_STATUS_SUCCESS) {
      const char *status_msg = nullptr;
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_load_graph: %s\n", status_msg);
      graph_id = 0;
    }

    return py::make_tuple(ret, graph_id);
  }

  /**
   * @brief This API loads an offline built and based on shared weight file from
   * file system.
   *
   * @param[in]  graph_bin Executable shared weight graph binary file path,
   * usually suffix is 'zip'
   * @param[in]  cfg  Configuration in loading graph stage with
   * `aipu_load_graph_cfg_wrapper_t` or dict dict key:
   *                      - 'wt_mem_region'
   *                      - 'wt_idxes'
   *                      - 'extra_weight_path'
   *                      - 'put_weight_gm'
   *                      - 'put_desc_gm'
   *                      - 'put_ws_gm'
   * @retval     tuple:
   *              - aipu_status_t: AIPU_STATUS_SUCCESS if success
   *              - graph_id0
   *              - graph_id1
   *              - ...
   */
  py::tuple aipu_load_share_weight_graph_py(const std::string &graph_bin,
                                            const py::handle &cfg) {
    std::vector<int32_t> idxes;
    std::string path;
    aipu_load_graph_cfg_t cfg_ = parse_graph_cfg(cfg, idxes, path);

    uint64_t *id = nullptr;
    uint32_t id_cnt = 0;
    aipu_status_t ret =
        aipu_load_share_weight_graph(m_ctx, graph_bin.c_str(), &id, &id_cnt,
                                     cfg.is_none() ? nullptr : &cfg_);
    if (ret != AIPU_STATUS_SUCCESS) {
      const char *status_msg = nullptr;
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_load_share_weight_graph: %s\n",
              status_msg);
      return py::make_tuple(ret, 0);
    }

    py::list lt;
    lt.append(ret);
    for (uint32_t i = 0; i < id_cnt; ++i)
      lt.append(id[i]);
    return py::tuple(lt);
  }

  /**
   * @brief This API is used to unload a loaded graph
   *
   * @param[in] graph_id Graph ID returned by aipu_load_graph
   *
   * @retval AIPU_STATUS_SUCCESS if success
   */
  aipu_status_t aipu_unload_graph_py(uint64_t graph_id) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    const char *status_msg = nullptr;

    ret = aipu_unload_graph(m_ctx, graph_id);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_unload_graph: %s\n", status_msg);
    }
    return ret;
  }

  /**
   * @brief This API is used to create a new job for a graph with provided
   * buffer handle.
   *
   * @param[in] graph_id: loaded graph id
   * @param[in] cfg: accept `aipu_create_job_cfg_wrapper_t`
   *
   * @retval    tuple
   *             - aipu_status_t: AIPU_STATUS_SUCCESS if success
   *             - job_id
   */
  py::tuple aipu_create_job_py(uint64_t graph_id, const py::handle &cfg) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    uint64_t job_id = 0;
    aipu_dynshape_param_t *ds_params = nullptr;

    if (!cfg.is_none()) {
      aipu_create_job_cfg_wrapper_t wrapper =
          py::cast<aipu_create_job_cfg_wrapper_t>(cfg);
      aipu_create_job_cfg_t cfg_ = {0};
      cfg_.partition_id = wrapper.partition_id;
      cfg_.dbg_dispatch = wrapper.dbg_dispatch;
      cfg_.dbg_core_id = wrapper.dbg_core_id;
      cfg_.qos_level = wrapper.qos_level;
      cfg_.fm_mem_region = wrapper.fm_mem_region;
      if (wrapper.fm_idxes.size() != 0) {
        cfg_.fm_idxes_cnt = wrapper.fm_idxes.size();
        cfg_.fm_idxes = wrapper.fm_idxes.data();
      }

      if (wrapper.dynshape.shape_items.size() != 0) {
        ds_params = new aipu_dynshape_param_t;
        ds_params->input_shape_cnt = wrapper.dynshape.shape_items.size();
        ds_params->shape_items =
            new aipu_dynshape_item_t[ds_params->input_shape_cnt];

        for (uint32_t i = 0; i < ds_params->input_shape_cnt; i++) {
          uint32_t shape_data_item_num = wrapper.dynshape.shape_items[i].size();

          ds_params->shape_items[i].ds_idx = i;
          ds_params->shape_items[i].ds_data = new uint32_t[shape_data_item_num];

          for (uint32_t j = 0; j < shape_data_item_num; j++)
            ds_params->shape_items[i].ds_data[j] =
                wrapper.dynshape.shape_items[i][j];
        }

        cfg_.dynshape = ds_params;
      }
      ret = aipu_create_job(m_ctx, graph_id, &job_id, &cfg_);
    } else {
      ret = aipu_create_job(m_ctx, graph_id, &job_id, nullptr);
    }

    const char *status_msg = nullptr;
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_finish_job: %s\n", status_msg);
      job_id = 0;
    }

    if (ds_params != nullptr) {
      for (uint32_t i = 0; i < ds_params->input_shape_cnt; i++) {
        delete[] ds_params->shape_items[i].ds_data;
        ds_params->shape_items[i].ds_data = nullptr;
      }

      delete[] ds_params->shape_items;
      ds_params->shape_items = nullptr;
      delete ds_params;
      ds_params = nullptr;
    }
    return py::make_tuple(ret, job_id);
  }

  /**
   * @brief This API is for specified job configuration, such as dump files and
   * simulator path
   * @param[in] job_id: created job id
   * @param[in] types: AIPU_CONFIG_TYPE_SIMULATION/AIPU_JOB_CONFIG_TYPE_DUMP_xxx
   * @param[in] cfg: accept `aipu_job_config_simulation_wrapper_t`,
   * `aipu_job_config_dump_wrapper_t` and `aipu_dynshape_param_wrapper_t`
   * aipu_job_config_simulation_wrapper_t: AIPU_CONFIG_TYPE_SIMULATION
   *              - data_dir: executable simulator file full path, only for
   * v1&v2 aipu_job_config_dump_wrapper_t: AIPU_JOB_CONFIG_TYPE_DUMP_xxx
   *              - dump_dir: dump_dir is used as file dump-path, dump required
   *              - prefix: name prefix of dump files, optional
   *              - output_prefix: name prefix of output dump files, optional
   *              - misc_prefix: name prefix of profile/printf data files,
   * optional aipu_dynshape_param_wrapper_t: AIPU_JOB_CONFIG_TYPE_DYNAMIC_PARAMS
   *              - shape_items: shape information of each input
   * @retval AIPU_STATUS_SUCCESS if success
   */
  aipu_status_t aipu_config_job_py(uint64_t job_id, uint64_t types,
                                   py::handle cfg) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    if (types == AIPU_CONFIG_TYPE_SIMULATION) {
      aipu_job_config_simulation_wrapper_t wrapper =
          py::cast<aipu_job_config_simulation_wrapper_t>(cfg);
      aipu_job_config_simulation_t cfg_ = {0};
      cfg_.data_dir = wrapper.data_dir.c_str();
      ret = aipu_config_job(m_ctx, job_id, types, &cfg_);
    } else if (types == AIPU_JOB_CONFIG_TYPE_DYNAMIC_PARAMS) {
      aipu_dynshape_param_wrapper_t wrapper =
          py::cast<aipu_dynshape_param_wrapper_t>(cfg);
      aipu_dynshape_param_t param_ = {0};
      std::vector<aipu_dynshape_item_t> items(wrapper.shape_items.size());
      for (uint32_t i = 0; i < items.size(); ++i) {
        items[i].ds_idx = i;
        items[i].ds_data = wrapper.shape_items[i].data();
      }
      param_.input_shape_cnt = items.size();
      param_.shape_items = items.data();
      ret = aipu_config_job(m_ctx, job_id, types, &param_);
    } else if ((types & 0x3FF) != (uint64_t)0) {
      aipu_job_config_dump_wrapper_t wrapper =
          py::cast<aipu_job_config_dump_wrapper_t>(cfg);
      aipu_job_config_dump_t cfg_ = {0};
      if (!wrapper.dump_dir.empty())
        cfg_.dump_dir = wrapper.dump_dir.c_str();
      if (!wrapper.prefix.empty())
        cfg_.prefix = wrapper.prefix.c_str();
      if (!wrapper.output_prefix.c_str())
        cfg_.output_prefix = wrapper.output_prefix.c_str();
      if (!wrapper.misc_prefix.c_str())
        cfg_.misc_prefix = wrapper.misc_prefix.c_str();
      ret = aipu_config_job(m_ctx, job_id, types, &cfg_);
    } else
      ret = AIPU_STATUS_ERROR_OP_NOT_SUPPORTED;

    const char *status_msg = nullptr;
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_config_job: %s\n", status_msg);
    }
    return ret;
  }

  /**
   * @brief This API is used to flush a new computation job onto AIPU (blocking)
   *
   * @param[in] job_id      Job ID returned by aipu_create_job
   * @param[in] time_out Time out (in millisecond) specified by application for
   * this job (A timeout of value <= 0 means an infinite timeout.)
   *
   * @retval AIPU_STATUS_SUCCESS if success
   */
  aipu_status_t aipu_finish_job_py(uint64_t job_id, int32_t time_out) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    const char *status_msg = nullptr;

    ret = aipu_finish_job(m_ctx, job_id, time_out);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_finish_job: %s\n", status_msg);
    }
    return ret;
  }

  /**
   * @brief This API is used to flush a new computation job onto AIPU
   * (non-blocking)
   *
   * @param[in] job_id      Job ID returned by aipu_create_job
   *
   * @retval AIPU_STATUS_SUCCESS if success
   *
   */
  aipu_status_t aipu_flush_job_py(uint64_t job_id,
                                  aipu_job_callback_func_t py_cb) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    const char *status_msg = nullptr;
    ret = aipu_flush_job(m_ctx, job_id, py_cb);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_flush_job: %s\n", status_msg);
    }
    return ret;
  }

  /**
   * @brief This API is used to get the execution status of a flushed job
   * (non-blocking)
   *
   * @param[in]  job_id    Job ID returned by aipu_create_job
   * @param[in]  timeout timeout value(ms) to poll job's status
   *                     timeout > 0: the max polling time window is 'timeout'
   *                     timeout = 0: non-blocking and return job's status
   * immediately. timeout = -1: blocking until job is really done or exception.
   *
   * @retval     tuple
   *              - aipu_status_t: AIPU_STATUS_SUCCESS if success
   *              - job status
   *
   * @note This API should be used by the application after aipu_flush_job
   * successfully returns.
   */
  py::tuple aipu_get_job_status_py(uint64_t job_id, int32_t timeout) {
    aipu_job_status_t status = AIPU_JOB_STATUS_NO_STATUS;
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    const char *status_msg = nullptr;

    ret = aipu_get_job_status(m_ctx, job_id, &status, timeout);

    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_get_job_status: %s\n", status_msg);
      status = AIPU_JOB_STATUS_NO_STATUS;
    }
    return py::make_tuple(ret, status);
  }

  /**
   * @brief This API is used to clean a finished job object
   *        scheduled by aipu_finish_job/aipu_flush_job
   *
   * @param[in] job_id Job ID returned by aipu_create_job
   *
   * @retval AIPU_STATUS_SUCCESS if success
   */
  aipu_status_t aipu_clean_job_py(uint64_t job_id) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    const char *status_msg = nullptr;

    ret = aipu_clean_job(m_ctx, job_id);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_clean_job: %s\n", status_msg);
    }
    return ret;
  }

  /**
   * @brief This API is used to get tensor count of specified type
   *
   * @param[in]  id   Job ID returned by aipu_create_job, or graph ID returned
   * by aipu_load_graph
   * @param[in]  type Tensor type
   *
   * @retval     tuple
   *              - aipu_status_t: AIPU_STATUS_SUCCESS if success
   *              - tensor number
   */
  py::tuple aipu_get_tensor_count_py(uint64_t graph_id,
                                     aipu_tensor_type_t type) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    const char *status_msg = nullptr;
    uint32_t num = 0;

    ret = aipu_get_tensor_count(m_ctx, graph_id, type, &num);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_get_tensor_count: %s\n", status_msg);
      num = 0;
    }

    return py::make_tuple(ret, num);
  }

  /**
   * @brief This API is used to get tensor's descriptor of specified type
   *
   * @param[in]  id     Job ID returned by aipu_create_job, or graph ID returned
   * by aipu_load_graph
   * @param[in]  type   Tensor type
   * @param[in]  tensor Tensor ID
   *
   * @retval normal tensor descriptor, otherwise a full zero descriptor
   * returned.
   */
  aipu_tensor_desc_t aipu_get_tensor_descriptor_py(uint64_t graph_id,
                                                   aipu_tensor_type_t type,
                                                   uint32_t tensor) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    const char *status_msg = nullptr;
    aipu_tensor_desc_t desc{0};

    ret = aipu_get_tensor_descriptor(m_ctx, graph_id, type, tensor, &desc);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_get_tensor_descriptor: %s\n",
              status_msg);
      return {0};
    }

    return desc;
  }

  /**
   * @brief This API is used to load input tensor data from input bin
   *
   * @param[in] job    Job ID returned by aipu_create_job
   * @param[in] tensor Input tensor ID
   * @param[in] filename   Input bin's full path
   *
   * @retval AIPU_STATUS_SUCCESS if success
   */
  aipu_status_t aipu_load_tensor_file_py(uint64_t job_id, uint32_t tensor,
                                         std::string &filename) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    const char *status_msg = nullptr;
    void *data = nullptr;
    aipu_tensor_desc_t desc;
    int fd = 0;

    ret = aipu_get_tensor_descriptor(m_ctx, job_id, AIPU_TENSOR_TYPE_INPUT,
                                     tensor, &desc);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_get_tensor_descriptor: %s\n",
              status_msg);
      return ret;
    }

    fd = open(filename.c_str(), O_RDONLY);
    if (fd <= 0) {
      fprintf(stderr, "[PY UMD ERROR] open file failed: %s! \n",
              filename.c_str());
      return AIPU_STATUS_ERROR_OPEN_FILE_FAIL;
    }

    data = mmap(nullptr, desc.size, PROT_READ, MAP_PRIVATE, fd, 0);
    if (MAP_FAILED == data) {
      fprintf(
          stderr,
          "[PY UMD ERROR] RT failed in mapping graph file: %s! (errno = %d)\n",
          filename.c_str(), errno);
      close(fd);
      return AIPU_STATUS_ERROR_MAP_FILE_FAIL;
    }

    ret = aipu_load_tensor(m_ctx, job_id, tensor, data);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_load_tensor: %s\n", status_msg);
    }

    munmap(data, desc.size);

    if (fd > 0)
      close(fd);

    return ret;
  }

  /**
   * @brief This API is used to load input tensor data from buffer
   *
   * @param[in] job    Job ID returned by aipu_create_job
   * @param[in] tensor Input tensor ID
   * @param[in] hdl It can byte `bytes`,`bytearray`,`numpy array`
   *
   * @retval AIPU_STATUS_SUCCESS if success
   */
  aipu_status_t aipu_load_tensor_py(uint64_t job_id, uint32_t tensor,
                                    py::handle hdl) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    aipu_tensor_desc_t desc;
    const char *status_msg = nullptr;

    const void *data = nullptr;
    if (py::isinstance<py::bytes>(hdl)) {
      m_data_type = DataType::BYTES;
      py::bytes by = py::cast<py::bytes>(hdl);
      data = PyBytes_AsString(by.ptr());
    } else if (py::isinstance<py::bytearray>(hdl)) {
      m_data_type = DataType::BYTE_ARRAY;
      py::buffer buffer = py::cast<py::buffer>(hdl);
      data = buffer.request().ptr;
    } else if (py::isinstance<py::array>(hdl)) {
      m_data_type = DataType::NUMPY_ARRAY;
      auto arr = py::array::ensure(hdl);
      if (!arr) {
        fprintf(stderr, "[PY UMD ERROR] numpy array is invalid\n");
        return AIPU_STATUS_ERROR_NULL_PTR;
      }
      constexpr int C_CONTIGUOUS =
          py::detail::npy_api::constants::NPY_ARRAY_C_CONTIGUOUS_;
      if (C_CONTIGUOUS != (arr.flags() & C_CONTIGUOUS)) {
        auto np = py::module::import("numpy");
        auto result = np.attr("ascontiguousarray")(arr);
        arr = pybind11::array::ensure(result);
      }
      data = arr.data();
    } else {
      fprintf(stderr, "[PY UMD ERROR] input tensor only supports "
                      "bytes,bytearray or numpy array\n");
      return AIPU_STATUS_ERROR_READ_FILE_FAIL;
    }

    ret = aipu_get_tensor_descriptor(m_ctx, job_id, AIPU_TENSOR_TYPE_INPUT,
                                     tensor, &desc);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_get_tensor_descriptor: %s\n",
              status_msg);
      return ret;
    }

    ret = aipu_load_tensor(m_ctx, job_id, tensor, data);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_load_tensor: %s\n", status_msg);
    }

    return ret;
  }

  /**
   * @brief This API is used to get tensor data of specified type
   *
   * @param[in]  job    Job ID returned by aipu_create_job
   * @param[in]  type   Tensor type
   * @param[in]  tensor Input tensor ID
   *
   * @retval     tuple
   *              - aipu_status_t: AIPU_STATUS_SUCCESS if success
   *              - py::none/py::bytes/py::bytearray/py::array: which depends on
   * input tensor data type
   *
   */
  py::tuple aipu_get_tensor_py(uint64_t job_id, aipu_tensor_type_t type,
                               uint32_t tensor) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    const char *status_msg = nullptr;
    aipu_tensor_desc_t desc;

    ret = aipu_get_tensor_descriptor(m_ctx, job_id, type, tensor, &desc);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_get_tensor_descriptor: %s\n",
              status_msg);
      return py::make_tuple(ret, py::none());
    }

    std::vector<uint8_t> out_data(desc.size, 0);
    ret = aipu_get_tensor(m_ctx, job_id, type, tensor, out_data.data());
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_get_tensor: %s\n", status_msg);
      return py::make_tuple(ret, py::none());
    }

    if (m_data_type == DataType::BYTE_ARRAY) {
      return py::make_tuple(
          ret,
          py::bytearray(reinterpret_cast<char *>(out_data.data()), desc.size));
    } else if (m_data_type == DataType::NUMPY_ARRAY) {
      return py::make_tuple(
          ret,
          py::array_t<uint8_t>({desc.size}, out_data.data(),
                               py::capsule())); /* python copy this buffer */
    }
    return py::make_tuple(
        ret, py::bytes(reinterpret_cast<char *>(out_data.data()), desc.size));
  }

  /**
   * @brief This API is used to send specific command to NPU driver, different
   * length of tuple, user would be better to get length of return value
   * @param[in] cmd cmd
   * @param[inout] py_arg input or output argument according to 'cmd'
   *
   * @retval tuple
   * @todo shared buf/dma-buf buffers used by python side properly
   * @note arguments and return description
   *              - AIPU_IOCTL_SET_PROFILE
   *                  dynamically enable/disable profiling feature of aipu v3
   * simulation kwargs: dict or unpacking arguments
   *                   - "set_profile": int
   *                  ret: tuple of 'aipu_status_t'
   *              - AIPU_IOCTL_GET_AIPUBIN_BUILDVERSION
   *                  get model binary's build version
   *                  kwargs: dict or unpacking arguments
   *                   - "graph_id": int
   *                  ret: tuple
   *                   - status: aipu_status_t
   *                   - version: int
   *              - AIPU_IOCTL_GET_DS_RANK
   *                  get rank of specified input of graph id
   *                  kwargs: dict or unpacking arguments
   *                   - "graph_id": int
   *                   - "ds_idx": int
   *                  ret: tuple
   *                   - status: aipu_status_t
   *                   - rank: int
   *              - AIPU_IOCTL_GET_DS_DIM_CONSTRAINT
   *                  get min/max dimensions of specified input of graph id
   *                  kwargs: dict or unpacking arguments
   *                   - "graph_id": int
   *                   - "ds_idx": int
   *                  ret: tuple
   *                   - min: list
   *                   - max: list
   *              - AIPU_IOCTL_SET_DYNAMIC_ASID1 (only for v3)
   *                  only for v3 core bind and multi-model parallel, default is
   * false, kwargs: dict or unpacking arguments
   *                   - "dynamic_asid1": True, False
   *                  ret: tuple of 'aipu_status_t'
   *              - AIPU_IOCTL_ALLOC_SHARE_BUF
   *                  request a shared buffer
   *                  kwargs: dict or unpacking arguments
   *                   - "size": int value
   *                   - "mem_type"(optional): int value of 'aipu_mem_region_t'
   *                  ret: tuple
   *                   - status: aipu_status_t
   *                   - pa: int
   *                   - 1D 'size' numpy array
   *              - AIPU_IOCTL_FREE_SHARE_BUF
   *                  request a shared buffer
   *                  kwargs: dict or unpacking arguments
   *                   - "pa": int
   *                   - "size": int
   *                  ret: tuple of aipu_status_t
   *              - AIPU_IOCTL_ALLOC_DMABUF
   *                  request dma_buf from KMD
   *                  kwargs: dict or unpacking arguments
   *                   - "bytes": int
   *                  ret: tuple
   *                   - status: aipu_status_t
   *                   - dma-buf fd: int
   *              - AIPU_IOCTL_FREE_DMABUF
   *                  kwargs: dict or unpacking arguments
   *                   - "dmabuf_fd": int
   *                  ret: tuple of 'aipu_status_t'
   *              - AIPU_IOCTL_GET_DMABUF_INFO
   *                  get dma_buf infomation, which includes pa and bytes by
   * provide dma_buf fd, and 'pa'/'bytes' can use to bind io buffer by calling
   * 'aipu_specify_iobuf' kwargs: dict or unpacking arguments
   *                   - "dmabuf_fd": int
   *                  ret:
   *                   - status: aipu_status_t
   *                   - pa: int
   *                   - bytes: int
   *              - AIPU_IOCTL_ATTACH_DMABUF
   *                  attach a dma_buf's fd exported from other modules. once
   * attached, driver will return 'pa' and 'bytes', which can bind to io buffer
   * by calling 'aipu_specify_iobuf'. If user wants to access this dma-buf, uses
   * 'mmap' kwargs: dict or unpacking arguments
   *                   - "dmabuf_fd": int, exported by other module
   *                  ret: tuple
   *                   - status: aipu_status_t
   *                   - pa: int
   *                   - bytes: int
   *              - AIPU_IOCTL_DETACH_DMABUF
   *                  detatch a dma_buf based on its fd
   *                  kwargs: dict or unpacking arguments
   *                   - "dmabuf_fd": int, exported by other module
   *                  ret: tuple of 'aipu_status_t'
   *              - AIPU_IOCTL_GET_VERSION
   *                  kwargs: no
   *                  ret: tuple
   *                   - status: aipu_status_t
   *                   - umd version: string
   *                   - kmd version: string
   *              - AIPU_IOCTL_ABORT_CMDPOOL
   *                  abort hw command pool manually, v3_2 command pool will be
   * destroyed automatically when all jobs done, then if you try to abort
   * command pool, it will return fail, only for >=v3 kwargs: no ret: tuple of
   * 'aipu_status_t'
   *              - AIPU_IOCTL_ENABLE_TICKCOUNTER:
   *                  enable performance counter
   *                  kwargs: no
   *                  ret: tuple of 'aipu_status_t'
   *              - AIPU_IOCTL_DISABLE_TICKCOUNTER:
   *                  disable performance counter
   *                  kwargs: no
   *                  ret: tuple of 'aipu_status_t'
   */
  py::tuple
  aipu_ioctl_py(uint32_t cmd, /*const*/ py::kwargs
                    kwargs) /* py::kwargs doesn't implement at(),find() */
  {
    auto check_args = [](const std::vector<const char *> &require,
                         const py::kwargs &kwargs) -> bool {
      for (auto &r : require) {
        if (!kwargs.contains(r)) {
          fprintf(stderr, "[PY UMD ERROR] please provide kwargs %s\n", r);
          return false;
        }
      }
      return true;
    };

    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    switch (cmd) {
    case AIPU_IOCTL_SET_PROFILE: {
      const std::vector<const char *> require = {"set_profile"};
      if (!check_args(require, kwargs))
        return py::make_tuple(AIPU_STATUS_ERROR_INVALID_CONFIG);

      uint32_t profile = kwargs[require[0]].cast<uint32_t>();
      ret = aipu_ioctl(m_ctx, cmd, &profile);
      return py::make_tuple(ret);
    }

    case AIPU_IOCTL_GET_AIPUBIN_BUILDVERSION: {
      const std::vector<const char *> require = {"graph_id"};
      if (!check_args(require, kwargs))
        return py::make_tuple(AIPU_STATUS_ERROR_INVALID_CONFIG, 0);

      aipu_bin_buildversion_t version = {};
      version.graph_id = kwargs[require[0]].cast<uint64_t>();
      ret = aipu_ioctl(m_ctx, cmd, &version);
      return py::make_tuple(ret, version.aipubin_buildversion);
    }

    case AIPU_IOCTL_GET_DS_RANK: {
      const std::vector<const char *> require = {"graph_id", "ds_idx"};
      if (!check_args(require, kwargs))
        return py::make_tuple(AIPU_STATUS_ERROR_INVALID_CONFIG, 0);

      aipu_dynshape_rank_t rank = {};
      rank.graph_id = kwargs[require[0]].cast<uint64_t>();
      rank.ds_idx = kwargs[require[1]].cast<uint64_t>();
      ret = aipu_ioctl(m_ctx, cmd, &rank);
      return py::make_tuple(ret, rank.rank);
    }

    case AIPU_IOCTL_GET_DS_DIM_CONSTRAINT: {
      const std::vector<const char *> require = {"graph_id", "ds_idx"};
      if (!check_args(require, kwargs))
        return py::make_tuple(AIPU_STATUS_ERROR_INVALID_CONFIG,
                              std::vector<uint32_t>{0},
                              std::vector<uint32_t>{0});

      aipu_dynshape_rank_t rank = {};
      rank.graph_id = kwargs[require[0]].cast<uint64_t>();
      rank.ds_idx = kwargs[require[1]].cast<uint64_t>();
      ret = aipu_ioctl(m_ctx, AIPU_IOCTL_GET_DS_RANK, &rank);
      if (ret != AIPU_STATUS_SUCCESS)
        return py::make_tuple(ret, std::vector<uint32_t>{0},
                              std::vector<uint32_t>{0});

      std::vector<uint32_t> min(rank.rank, 0);
      std::vector<uint32_t> max(rank.rank, 0);
      aipu_dynshape_dimension_t dyn_dim = {};
      dyn_dim.graph_id = rank.graph_id;
      dyn_dim.ds_idx = rank.ds_idx;
      dyn_dim.min_dim = min.data();
      dyn_dim.max_dim = max.data();
      ret = aipu_ioctl(m_ctx, cmd, &dyn_dim);
      return py::make_tuple(ret, min, max);
    }

    case AIPU_IOCTL_SET_DYNAMIC_ASID1: {
      const std::vector<const char *> require = {"dynamic_asid1"};
      if (!check_args(require, kwargs))
        return py::make_tuple(AIPU_STATUS_ERROR_INVALID_CONFIG);

      bool dynamic_asid1 = kwargs[require[0]].cast<bool>();
      ret = aipu_ioctl(m_ctx, cmd, &dynamic_asid1);
      return py::make_tuple(ret);
    }

    case AIPU_IOCTL_ALLOC_SHARE_BUF: {
      const std::vector<const char *> require = {"size"};
      if (!check_args(require, kwargs))
        return py::make_tuple(AIPU_STATUS_ERROR_INVALID_CONFIG, 0, 0);

      aipu_share_buf_t share_buf = {};
      share_buf.size = kwargs[require[0]].cast<uint32_t>();
      share_buf.mem_type = AIPU_MEM_REGION_DEFAULT;
      if (kwargs.size() > 1)
        share_buf.mem_type = kwargs[require[1]].cast<uint32_t>();

      ret = aipu_ioctl(m_ctx, cmd, &share_buf);
      auto capsule = py::capsule((uint8_t *)share_buf.va,
                                 [](void *p) {}); /* driver owner this buffer */
      return py::make_tuple(ret, share_buf.pa,
                            py::array_t<uint8_t>({share_buf.size},
                                                 (uint8_t *)share_buf.va,
                                                 capsule));
    }

    case AIPU_IOCTL_FREE_SHARE_BUF: {
      const std::vector<const char *> require = {"pa", "size"};
      if (!check_args(require, kwargs))
        return py::make_tuple(AIPU_STATUS_ERROR_INVALID_CONFIG);

      aipu_share_buf_t share_buf = {};
      share_buf.pa = kwargs[require[0]].cast<uint64_t>();
      share_buf.size = kwargs[require[1]].cast<uint32_t>();
      ret = aipu_ioctl(m_ctx, cmd, &share_buf);
      return py::make_tuple(ret);
    }

    case AIPU_IOCTL_ALLOC_DMABUF: {
      const std::vector<const char *> require = {"bytes"};
      if (!check_args(require, kwargs))
        return py::make_tuple(AIPU_STATUS_ERROR_INVALID_CONFIG, 0);

      aipu_dma_buf_req_t req = {};
      req.bytes = kwargs[require[0]].cast<uint32_t>();
      ret = aipu_ioctl(m_ctx, cmd, &req);
      return py::make_tuple(ret, req.fd);
    }

    case AIPU_IOCTL_GET_DMABUF_INFO:
    case AIPU_IOCTL_ATTACH_DMABUF: {
      const std::vector<const char *> require = {"dmabuf_fd"};
      if (!check_args(require, kwargs))
        return py::make_tuple(AIPU_STATUS_ERROR_INVALID_CONFIG, 0, 0);

      aipu_dma_buf_desc desc;
      desc.fd = kwargs[require[0]].cast<int32_t>();
      ret = aipu_ioctl(m_ctx, cmd, &desc);
      return py::make_tuple(ret, desc.pa, desc.bytes);
    }

    case AIPU_IOCTL_FREE_DMABUF:
    case AIPU_IOCTL_DETACH_DMABUF: {
      const std::vector<const char *> require = {"dmabuf_fd"};
      if (!check_args(require, kwargs))
        return py::make_tuple(AIPU_STATUS_ERROR_INVALID_CONFIG);

      int32_t fd = kwargs[require[0]].cast<int32_t>();
      ret = aipu_ioctl(m_ctx, cmd, &fd);
      return py::make_tuple(ret);
    }

    case AIPU_IOCTL_GET_VERSION: {
      aipu_driver_version version = {};
      ret = aipu_ioctl(m_ctx, cmd, &version);
      return py::make_tuple(ret, std::string(version.umd_version),
                            std::string(version.kmd_version));
    }

    case AIPU_IOCTL_ABORT_CMDPOOL:
    case AIPU_IOCTL_ENABLE_TICKCOUNTER:
    case AIPU_IOCTL_DISABLE_TICKCOUNTER: {
      ret = aipu_ioctl(m_ctx, cmd, nullptr);
      return py::make_tuple(ret);
    }

    default:;
    }

    fprintf(stderr, "[PY UMD ERROR] aipu_ioctl invalid cmd %u\n", cmd);
    return py::make_tuple(AIPU_STATUS_ERROR_INVALID_OP);
  }

  /**
   * @brief This API is used to specify a shared buffer as job's io buffer.
   *
   * @param[in] job_id Job ID returned by aipu_create_job
   * @param[in] cfg accept 'aipu_shared_tensor_info_t'
   * @retval AIPU_STATUS_SUCCESS if success
   */
  aipu_status_t aipu_specify_iobuf_py(uint64_t job_id, const py::handle cfg) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    const char *status_msg = nullptr;
    aipu_shared_tensor_info_t info = py::cast<aipu_shared_tensor_info_t>(cfg);

    ret = aipu_specify_iobuf(m_ctx, job_id, &info);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_specify_iobuf: %s\n", status_msg);
    }

    return ret;
  }

  /**
   * @brief This API is used to create a batch queue
   *
   * @param[in] graph_id Graph ID returned by aipu_load_graph
   *
   * @retval tuple
   *          - aiput_status_t: AIPU_STATUS_SUCCESS if success
   *          - queue_id
   */
  py::tuple aipu_create_batch_queue_py(uint64_t graph_id) {
    uint32_t queue_id = 0;
    const char *status_msg = nullptr;

    aipu_status_t ret = aipu_create_batch_queue(m_ctx, graph_id, &queue_id);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_create_batch_queue: %s\n",
              status_msg);
    }
    return py::make_tuple(ret, queue_id);
  }

  /**
   * @brief This API is used to create a batch queue
   *
   * @param[in] graph_id Graph ID returned by aipu_load_graph
   * @param[in] queue_id Queue ID return by aipu_create_batch_queue
   *
   * @retval AIPU_STATUS_SUCCESS if success
   */
  aipu_status_t aipu_clean_batch_queue_py(uint64_t graph_id,
                                          uint32_t queue_id) {
    const char *status_msg = nullptr;

    aipu_status_t ret = aipu_clean_batch_queue(m_ctx, graph_id, queue_id);
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_clean_batch_queue: %s\n",
              status_msg);
    }
    return ret;
  }

  /**
   * @brief This API is used to do basic config for batch inference on simulator
   * and HW.
   *
   * @param[in] graph_id Graph ID returned by aipu_load_graph
   * @param[in] queue_id Queue ID return by aipu_create_batch_queue
   * @param[in] types: AIPU_CONFIG_TYPE_SIMULATION/AIPU_JOB_CONFIG_TYPE_DUMP_xxx
   * @param[in] cfg: accept `aipu_job_config_simulation_t` and
   * `aipu_job_config_dump_t` aipu_job_config_simulation_t:
   * AIPU_CONFIG_TYPE_SIMULATION
   *              - data_dir: executable simulator file full path, only for
   * v1&v2 aipu_job_config_dump_t: AIPU_JOB_CONFIG_TYPE_DUMP_xxx
   *              - dump_dir: dump_dir is used as file dump-path, dump required
   *              - prefix: name prefix of dump files, optional
   *              - output_prefix: name prefix of output dump files, optional
   *              - misc_prefix: name prefix of profile/printf data files,
   * optional
   *
   * @retval AIPU_STATUS_SUCCESS if success
   */
  aipu_status_t aipu_config_batch_dump_py(uint64_t graph_id, uint32_t queue_id,
                                          uint64_t types, py::handle cfg) {
    aipu_job_config_dump_wrapper_t wrapper =
        py::cast<aipu_job_config_dump_wrapper_t>(cfg);
    aipu_job_config_dump_t cfg_;
    memset(&cfg_, 0, sizeof(aipu_job_config_dump_t));
    if (!wrapper.dump_dir.empty())
      cfg_.dump_dir = wrapper.dump_dir.c_str();
    if (!wrapper.prefix.empty())
      cfg_.prefix = wrapper.prefix.c_str();
    if (!wrapper.output_prefix.c_str())
      cfg_.output_prefix = wrapper.output_prefix.c_str();
    if (!wrapper.misc_prefix.c_str())
      cfg_.misc_prefix = wrapper.misc_prefix.c_str();
    aipu_status_t ret =
        aipu_config_batch_dump(m_ctx, graph_id, queue_id, types, &cfg_);

    const char *status_msg = nullptr;
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_config_batch_dump: %s\n",
              status_msg);
    }
    return ret;
  }

  /**
   * @brief This API is used to add a group buffers of one frame inference to
   * batch queue.
   *
   * @param[in] graph_id Graph ID returned by aipu_load_graph
   * @param[in] queue_id Queue ID return by aipu_create_batch_queue
   * @param[in] inputs bytesarray or numpy arraym, bytes (bytes can only be
   * input, because bytes cannot be modified)
   * @param[in] outputs data type same with `inputs` would be bettern
   *
   * @retval AIPU_STATUS_SUCCESS if success
   */
  aipu_status_t aipu_add_batch_py(uint64_t graph_id, uint32_t queue_id,
                                  std::vector<py::handle> &inputs,
                                  std::vector<py::handle> &outputs) {
    enum class IoType {
      INPUT = 0x00,
      OUTPUT,
    };
    auto get_raw_ptr = [](py::handle &hdl, IoType io_type) -> char * {
      if (py::isinstance<py::bytes>(hdl)) {
        if (io_type == IoType::OUTPUT) {
          fprintf(stderr,
                  "[PY UMD ERROR] aipu_add_batch cannot accept `bytes` object "
                  "for output, provide bytearray or numpy array\n");
          return nullptr;
        }
        py::bytes by = py::cast<py::bytes>(hdl);
        return PyBytes_AsString(by.ptr());
      } else if (py::isinstance<py::bytearray>(
                     hdl)) /* no py::xx corresponding */
      {
        py::buffer buffer = py::cast<py::buffer>(hdl);
        return static_cast<char *>(buffer.request().ptr);
      } else if (py::isinstance<py::array>(hdl)) {
        auto arr = py::array::ensure(hdl);
        if (!arr) {
          fprintf(stderr, "[PY UMD ERROR] numpy array is invalid\n");
          return nullptr;
        }

        constexpr int C_CONTIGUOUS =
            py::detail::npy_api::constants::NPY_ARRAY_C_CONTIGUOUS_;
        if (C_CONTIGUOUS != (arr.flags() & C_CONTIGUOUS)) {
          auto np = py::module::import("numpy");
          auto result = np.attr("ascontiguousarray")(arr);
          arr = py::array::ensure(result);
        }
        return static_cast<char *>(arr.request().ptr);
      }

      fprintf(stderr, "[PY UMD ERROR] aipu_add_batch only accepts bytes, "
                      "bytearray or numpy array\n");
      return nullptr;
    };

    /* load inputs data & set output buffer, and pointer is valid always */
    std::vector<char *> inputs_buffer(inputs.size());
    for (uint32_t i = 0; i < inputs.size(); ++i) {
      char *ptr = get_raw_ptr(inputs[i], IoType::INPUT);
      if (ptr == nullptr)
        return AIPU_STATUS_ERROR_NULL_PTR;
      inputs_buffer[i] = ptr;
    }

    std::vector<char *> outputs_buffer(outputs.size());
    for (uint32_t i = 0; i < outputs.size(); ++i) {
      char *ptr = get_raw_ptr(outputs[i], IoType::OUTPUT);
      if (ptr == nullptr)
        return AIPU_STATUS_ERROR_NULL_PTR;
      outputs_buffer[i] = ptr;
    }

    const char *status_msg = nullptr;
    aipu_status_t ret = aipu_add_batch(
        m_ctx, graph_id, queue_id, inputs_buffer.data(), outputs_buffer.data());
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_config_batch_dump: %s\n",
              status_msg);
    }
    return ret;
  }

  /**
   * @brief This API is used to run multiple batch inference.
   *
   * @param[in] graph_id Graph ID returned by aipu_load_graph
   * @param[in] queue_id Queue ID return by aipu_create_batch_queue
   * @param[in] cfg: accept `aipu_create_job_cfg_wrapper_t`
   *
   * @retval AIPU_STATUS_SUCCESS if success
   */
  aipu_status_t aipu_finish_batch_py(uint64_t graph_id, uint32_t queue_id,
                                     const py::handle cfg) {
    aipu_status_t ret = AIPU_STATUS_SUCCESS;
    aipu_dynshape_param_t *ds_params = nullptr;

    if (!cfg.is_none()) {
      aipu_create_job_cfg_wrapper_t wrapper =
          py::cast<aipu_create_job_cfg_wrapper_t>(cfg);
      aipu_create_job_cfg_t cfg_ = {0};
      cfg_.partition_id = wrapper.partition_id;
      cfg_.dbg_dispatch = wrapper.dbg_dispatch;
      cfg_.dbg_core_id = wrapper.dbg_core_id;
      cfg_.qos_level = wrapper.qos_level;
      cfg_.fm_mem_region = wrapper.fm_mem_region;
      if (wrapper.fm_idxes.size() != 0) {
        cfg_.fm_idxes_cnt = wrapper.fm_idxes.size();
        cfg_.fm_idxes = wrapper.fm_idxes.data();
      }

      if (wrapper.dynshape.shape_items.size() != 0) {
        ds_params = new aipu_dynshape_param_t;
        ds_params->input_shape_cnt = wrapper.dynshape.shape_items.size();
        ds_params->shape_items =
            new aipu_dynshape_item_t[ds_params->input_shape_cnt];

        for (uint32_t i = 0; i < ds_params->input_shape_cnt; i++) {
          uint32_t shape_data_item_num = wrapper.dynshape.shape_items[i].size();

          ds_params->shape_items[i].ds_idx = i;
          ds_params->shape_items[i].ds_data = new uint32_t[shape_data_item_num];

          for (uint32_t j = 0; j < shape_data_item_num; j++)
            ds_params->shape_items[i].ds_data[j] =
                wrapper.dynshape.shape_items[i][j];
        }

        cfg_.dynshape = ds_params;
      }
      ret = aipu_finish_batch(m_ctx, graph_id, queue_id, &cfg_);
    } else {
      ret = aipu_finish_batch(m_ctx, graph_id, queue_id, nullptr);
    }

    const char *status_msg = nullptr;
    if (ret != AIPU_STATUS_SUCCESS) {
      aipu_get_error_message(m_ctx, ret, &status_msg);
      fprintf(stderr, "[PY UMD ERROR] aipu_finish_batch: %s\n", status_msg);
    }

    if (ds_params != nullptr) {
      for (uint32_t i = 0; i < ds_params->input_shape_cnt; i++) {
        delete[] ds_params->shape_items[i].ds_data;
        ds_params->shape_items[i].ds_data = nullptr;
      }

      delete[] ds_params->shape_items;
      ds_params->shape_items = nullptr;
      delete ds_params;
      ds_params = nullptr;
    }
    return ret;
  }

public:
  NPU(){};
  NPU(const NPU &aipu) = delete;
  NPU &operator=(const NPU &aipu) = delete;

  virtual ~NPU(){};

private:
  /* when user loads input tensor, NPU will try to return same data type output
   * tensor, and only mark last one input tensor */
  enum class DataType {
    BYTES = 0x00,
    BYTE_ARRAY,
    NUMPY_ARRAY,
  };

  aipu_ctx_handle_t *m_ctx = nullptr;
  DataType m_data_type = DataType::BYTES;
};
} // namespace aipudrv

using namespace aipudrv;
/**
 * Decare entry module: libaipudriv
 */
PYBIND11_MODULE(libaipudrv, m) {
  m.doc() = "CompassNPU UMD python APIs";

  py::class_<aipu_ctx_handle_t>(m, "aipu_ctx_handle_t")
      .def(py::init<>())
      .def_readwrite("handle", &aipu_ctx_handle_t::handle);

  py::enum_<device_status_t>(m, "device_status_t")
      .value("DEV_IDLE", device_status_t::DEV_IDLE)
      .value("DEV_BUSY", device_status_t::DEV_BUSY)
      .value("DEV_EXCEPTION", device_status_t::DEV_EXCEPTION)
      .export_values();

  py::enum_<aipu_data_type_t>(m, "aipu_data_type_t")
      .value("AIPU_DATA_TYPE_NONE", aipu_data_type_t::AIPU_DATA_TYPE_NONE)
      .value("AIPU_DATA_TYPE_BOOL", aipu_data_type_t::AIPU_DATA_TYPE_BOOL)
      .value("AIPU_DATA_TYPE_U8", aipu_data_type_t::AIPU_DATA_TYPE_U8)
      .value("AIPU_DATA_TYPE_S8", aipu_data_type_t::AIPU_DATA_TYPE_S8)
      .value("AIPU_DATA_TYPE_U16", aipu_data_type_t::AIPU_DATA_TYPE_U16)
      .value("AIPU_DATA_TYPE_S16", aipu_data_type_t::AIPU_DATA_TYPE_S16)
      .value("AIPU_DATA_TYPE_U32", aipu_data_type_t::AIPU_DATA_TYPE_U32)
      .value("AIPU_DATA_TYPE_S32", aipu_data_type_t::AIPU_DATA_TYPE_S32)
      .value("AIPU_DATA_TYPE_U64", aipu_data_type_t::AIPU_DATA_TYPE_U64)
      .value("AIPU_DATA_TYPE_S64", aipu_data_type_t::AIPU_DATA_TYPE_S64)
      .value("AIPU_DATA_TYPE_F16", aipu_data_type_t::AIPU_DATA_TYPE_F16)
      .value("AIPU_DATA_TYPE_F32", aipu_data_type_t::AIPU_DATA_TYPE_F32)
      .value("AIPU_DATA_TYPE_F64", aipu_data_type_t::AIPU_DATA_TYPE_F64)
      .value("AIPU_DATA_TYPE_BF16", aipu_data_type_t::AIPU_DATA_TYPE_BF16)

      /* byte-aligned u/int4 */
      .value("AIPU_DATA_TYPE_ALIGNED_U4",
             aipu_data_type_t::AIPU_DATA_TYPE_ALIGNED_U4)
      .value("AIPU_DATA_TYPE_ALIGNED_S4",
             aipu_data_type_t::AIPU_DATA_TYPE_ALIGNED_S4)

      /* byte-aligned u/int12 */
      .value("AIPU_DATA_TYPE_ALIGNED_U12",
             aipu_data_type_t::AIPU_DATA_TYPE_ALIGNED_U12)
      .value("AIPU_DATA_TYPE_ALIGNED_S12",
             aipu_data_type_t::AIPU_DATA_TYPE_ALIGNED_S12)
      .value("AIPU_DATA_TYPE_COMPACT_U4",
             aipu_data_type_t::AIPU_DATA_TYPE_COMPACT_U4)
      .value("AIPU_DATA_TYPE_COMPACT_S4",
             aipu_data_type_t::AIPU_DATA_TYPE_COMPACT_S4)
      .value("AIPU_DATA_TYPE_COMPACT_U12",
             aipu_data_type_t::AIPU_DATA_TYPE_COMPACT_U12)
      .value("AIPU_DATA_TYPE_COMPACT_S12",
             aipu_data_type_t::AIPU_DATA_TYPE_COMPACT_S12)
      .export_values();

  py::enum_<aipu_tensor_type_t>(m, "aipu_tensor_type_t")
      .value("AIPU_TENSOR_TYPE_INPUT",
             aipu_tensor_type_t::AIPU_TENSOR_TYPE_INPUT)
      .value("AIPU_TENSOR_TYPE_OUTPUT",
             aipu_tensor_type_t::AIPU_TENSOR_TYPE_OUTPUT)
      .value("AIPU_TENSOR_TYPE_INTER_DUMP",
             aipu_tensor_type_t::AIPU_TENSOR_TYPE_INTER_DUMP)
      .value("AIPU_TENSOR_TYPE_PRINTF",
             aipu_tensor_type_t::AIPU_TENSOR_TYPE_PRINTF)
      .value("AIPU_TENSOR_TYPE_PROFILER",
             aipu_tensor_type_t::AIPU_TENSOR_TYPE_PROFILER)
      .value("AIPU_TENSOR_TYPE_LAYER_COUNTER",
             aipu_tensor_type_t::AIPU_TENSOR_TYPE_LAYER_COUNTER)
      /* only for v1v2 */
      .value("AIPU_TENSOR_TYPE_ERROR_CODE",
             aipu_tensor_type_t::AIPU_TENSOR_TYPE_ERROR_CODE)
      .value("AIPU_TENSOR_TYPE_OUT_TENSOR_SHAPE",
             aipu_tensor_type_t::AIPU_TENSOR_TYPE_OUT_TENSOR_SHAPE)
      .export_values();

  py::class_<aipu_tensor_desc_t>(m, "aipu_tensor_desc_t")
      .def(py::init<>())
      .def_readwrite("id", &aipu_tensor_desc_t::id)
      .def_readwrite("size", &aipu_tensor_desc_t::size)
      .def_readwrite("scale", &aipu_tensor_desc_t::scale)
      .def_readwrite("zero_point", &aipu_tensor_desc_t::zero_point)
      .def_readwrite("data_type", &aipu_tensor_desc_t::data_type);

  py::enum_<aipu_job_status_t>(m, "aipu_job_status_t")
      .value("AIPU_JOB_STATUS_NO_STATUS",
             aipu_job_status_t::AIPU_JOB_STATUS_NO_STATUS)
      .value("AIPU_JOB_STATUS_DONE", aipu_job_status_t::AIPU_JOB_STATUS_DONE)
      .value("AIPU_JOB_STATUS_EXCEPTION",
             aipu_job_status_t::AIPU_JOB_STATUS_EXCEPTION)
      .export_values();

#if 0
    py::class_<aipu_debugger_job_info_t>(m, "aipu_debugger_job_info_t")
        .def(py::init<>())
        .def_readonly("instr_base", &aipu_debugger_job_info_t::instr_base)
        .def_readwrite("simulation_aipu", &aipu_debugger_job_info_t::simulation_aipu)
        .def_readwrite("simulation_mem_engine", &aipu_debugger_job_info_t::simulation_mem_engine);
#endif

  py::enum_<aipu_config_type_t>(m, "aipu_config_type_t")
      .value("AIPU_JOB_CONFIG_TYPE_DUMP_TEXT",
             aipu_config_type_t::AIPU_JOB_CONFIG_TYPE_DUMP_TEXT)
      .value("AIPU_JOB_CONFIG_TYPE_DUMP_WEIGHT",
             aipu_config_type_t::AIPU_JOB_CONFIG_TYPE_DUMP_WEIGHT)
      .value("AIPU_JOB_CONFIG_TYPE_DUMP_RODATA",
             aipu_config_type_t::AIPU_JOB_CONFIG_TYPE_DUMP_RODATA)
      .value("AIPU_JOB_CONFIG_TYPE_DUMP_DESCRIPTOR",
             aipu_config_type_t::AIPU_JOB_CONFIG_TYPE_DUMP_DESCRIPTOR)
      .value("AIPU_JOB_CONFIG_TYPE_DUMP_INPUT",
             aipu_config_type_t::AIPU_JOB_CONFIG_TYPE_DUMP_INPUT)
      .value("AIPU_JOB_CONFIG_TYPE_DUMP_OUTPUT",
             aipu_config_type_t::AIPU_JOB_CONFIG_TYPE_DUMP_OUTPUT)
      .value("AIPU_JOB_CONFIG_TYPE_DUMP_REUSE",
             aipu_config_type_t::AIPU_JOB_CONFIG_TYPE_DUMP_REUSE)
      .value("AIPU_JOB_CONFIG_TYPE_DUMP_TCB_CHAIN",
             aipu_config_type_t::AIPU_JOB_CONFIG_TYPE_DUMP_TCB_CHAIN)
      .value("AIPU_JOB_CONFIG_TYPE_DUMP_EMULATION",
             aipu_config_type_t::AIPU_JOB_CONFIG_TYPE_DUMP_EMULATION)
      .value("AIPU_JOB_CONFIG_TYPE_DUMP_PROFILE",
             aipu_config_type_t::AIPU_JOB_CONFIG_TYPE_DUMP_PROFILE)
      .value("AIPU_JOB_CONFIG_TYPE_DYNAMIC_PARAMS",
             aipu_config_type_t::AIPU_JOB_CONFIG_TYPE_DYNAMIC_PARAMS)
      .value("AIPU_CONFIG_TYPE_SIMULATION",
             aipu_config_type_t::AIPU_CONFIG_TYPE_SIMULATION)
      .value("AIPU_CONFIG_TYPE_HW", aipu_config_type_t::AIPU_CONFIG_TYPE_HW)
      .value("AIPU_GLOBAL_CONFIG_TYPE_DISABLE_VER_CHECK",
             aipu_config_type_t::AIPU_GLOBAL_CONFIG_TYPE_DISABLE_VER_CHECK)
      .value("AIPU_GLOBAL_CONFIG_TYPE_ENABLE_VER_CHECK",
             aipu_config_type_t::AIPU_GLOBAL_CONFIG_TYPE_ENABLE_VER_CHECK)
      .export_values();

  py::class_<aipu_job_config_dump_wrapper_t>(m, "aipu_job_config_dump_t")
      .def(py::init<>())
      .def_readwrite("dump_dir", &aipu_job_config_dump_wrapper_t::dump_dir)
      .def_readwrite("prefix", &aipu_job_config_dump_wrapper_t::prefix)
      .def_readwrite("output_prefix",
                     &aipu_job_config_dump_wrapper_t::output_prefix)
      .def_readwrite("misc_prefix",
                     &aipu_job_config_dump_wrapper_t::misc_prefix);

  py::class_<aipu_job_config_simulation_wrapper_t>(
      m, "aipu_job_config_simulation_t")
      .def(py::init<>())
      .def_readwrite("data_dir",
                     &aipu_job_config_simulation_wrapper_t::data_dir);

  py::class_<aipu_global_config_simulation_wrapper_t>(
      m, "aipu_global_config_simulation_t")
      .def(py::init<>())
      .def_readwrite("simulator",
                     &aipu_global_config_simulation_wrapper_t::simulator)
      .def_readwrite("log_file_path",
                     &aipu_global_config_simulation_wrapper_t::log_file_path)
      .def_readwrite("npu_arch_desc",
                     &aipu_global_config_simulation_wrapper_t::npu_arch_desc)
      .def_readwrite("plugin_name",
                     &aipu_global_config_simulation_wrapper_t::plugin_name)
      .def_readwrite("json_filename",
                     &aipu_global_config_simulation_wrapper_t::json_filename)
      .def_readwrite("log_level",
                     &aipu_global_config_simulation_wrapper_t::log_level)
      .def_readwrite("gm_size",
                     &aipu_global_config_simulation_wrapper_t::gm_size)
      .def_readwrite("verbose",
                     &aipu_global_config_simulation_wrapper_t::verbose)
      .def_readwrite("enable_avx",
                     &aipu_global_config_simulation_wrapper_t::enable_avx)
      .def_readwrite("enable_calloc",
                     &aipu_global_config_simulation_wrapper_t::enable_calloc)
      .def_readwrite("en_eval",
                     &aipu_global_config_simulation_wrapper_t::en_eval)
      .def_readwrite("en_l2d", &aipu_global_config_simulation_wrapper_t::en_l2d)
      .def_readwrite("en_fast_perf",
                     &aipu_global_config_simulation_wrapper_t::en_fast_perf)
      .def_readwrite("freq_mhz",
                     &aipu_global_config_simulation_wrapper_t::freq_mhz)
      .def_readwrite("ddr_latency_rd",
                     &aipu_global_config_simulation_wrapper_t::ddr_latency_rd)
      .def_readwrite("ddr_latency_wr",
                     &aipu_global_config_simulation_wrapper_t::ddr_latency_wr)
      .def_readwrite("ddr_bw", &aipu_global_config_simulation_wrapper_t::ddr_bw)
      .def_readwrite("ddr_bw_ratio",
                     &aipu_global_config_simulation_wrapper_t::ddr_bw_ratio)
      .def_readwrite("perf_report",
                     &aipu_global_config_simulation_wrapper_t::perf_report);

  py::class_<aipu_global_config_hw_t>(m, "aipu_global_config_hw_t")
      .def(py::init<>())
      .def_readwrite("poll_in_commit_thread",
                     &aipu_global_config_hw_t::poll_in_commit_thread);

#if 0
    py::class_<callback_args_t>(m, "callback_args_t")
        .def(py::init<>())
        .def_readwrite("func_arg", &callback_args_t::func_arg)
        .def_readwrite("job_id", &callback_args_t::job_id)
        .def_readwrite("job_state", &callback_args_t::job_state);

    py::class_<callback_wrapper_t>(m, "callback_wrapper_t")
        .def(py::init<>())
        .def_readwrite("cb_func", &callback_wrapper_t::cb_func)
        .def_readwrite("cb_args", &callback_wrapper_t::cb_args);
#endif

  py::class_<aipu_core_info_t>(m, "aipu_core_info_t")
      .def(py::init<>())
      .def_readwrite("reg_base", &aipu_core_info_t::reg_base);

  py::class_<aipu_shared_tensor_info_t>(m, "aipu_shared_tensor_info_t")
      .def(py::init<>())
      .def_readwrite("type", &aipu_shared_tensor_info_t::type)
      .def_readwrite("tensor_idx", &aipu_shared_tensor_info_t::tensor_idx)
      .def_readwrite("id", &aipu_shared_tensor_info_t::id)
      .def_readwrite("pa", &aipu_shared_tensor_info_t::pa)
      .def_readwrite("dmabuf_fd", &aipu_shared_tensor_info_t::dmabuf_fd)
      .def_readwrite("offset_in_dmabuf",
                     &aipu_shared_tensor_info_t::offset_in_dmabuf)
      .def_readwrite("shared_case_type",
                     &aipu_shared_tensor_info_t::shared_case_type);

  py::class_<aipu_bin_buildversion_t>(m, "aipu_bin_buildversion_t")
      .def(py::init<>())
      .def_readwrite("graph_id", &aipu_bin_buildversion_t::graph_id)
      .def_readwrite("aipubin_buildversion",
                     &aipu_bin_buildversion_t::aipubin_buildversion);

  py::class_<aipu_dynshape_rank_wrapper_t>(m, "aipu_dynshape_rank_t")
      .def(py::init<>())
      .def_readwrite("graph_id", &aipu_dynshape_rank_wrapper_t::graph_id)
      .def_readwrite("ds_idx", &aipu_dynshape_rank_wrapper_t::ds_idx)
      .def_readwrite("rank", &aipu_dynshape_rank_wrapper_t::rank);

  py::class_<aipu_dynshape_dimension_wrapper_t>(m, "aipu_dynshape_dimension_t")
      .def(py::init<>())
      .def_readwrite("graph_id", &aipu_dynshape_dimension_wrapper_t::graph_id)
      .def_readwrite("ds_idx", &aipu_dynshape_dimension_wrapper_t::ds_idx)
      .def_readwrite("min_dim", &aipu_dynshape_dimension_wrapper_t::min_dim)
      .def_readwrite("max_dim", &aipu_dynshape_dimension_wrapper_t::max_dim);

  py::class_<aipu_dynshape_param_wrapper_t>(m, "aipu_dynshape_param_t")
      .def(py::init<>())
      .def_readwrite("shape_items",
                     &aipu_dynshape_param_wrapper_t::shape_items);

  py::class_<aipu_load_graph_cfg_wrapper_t>(m, "aipu_load_graph_cfg_t")
      .def(py::init<>())
      .def_readwrite("wt_mem_region",
                     &aipu_load_graph_cfg_wrapper_t::wt_mem_region)
      .def_readwrite("wt_idxes", &aipu_load_graph_cfg_wrapper_t::wt_idxes)
      .def_readwrite("extra_weight_path",
                     &aipu_load_graph_cfg_wrapper_t::extra_weight_path)
      .def_readwrite("put_weight_gm",
                     &aipu_load_graph_cfg_wrapper_t::put_weight_gm)
      .def_readwrite("put_desc_gm", &aipu_load_graph_cfg_wrapper_t::put_desc_gm)
      .def_readwrite("put_ws_gm", &aipu_load_graph_cfg_wrapper_t::put_ws_gm);

  py::class_<aipu_create_job_cfg_wrapper_t>(m, "aipu_create_job_cfg_t")
      .def(py::init<>())
      .def_readwrite("partition_id",
                     &aipu_create_job_cfg_wrapper_t::partition_id)
      .def_readwrite("dbg_dispatch",
                     &aipu_create_job_cfg_wrapper_t::dbg_dispatch)
      .def_readwrite("dbg_core_id", &aipu_create_job_cfg_wrapper_t::dbg_core_id)
      .def_readwrite("qos_level", &aipu_create_job_cfg_wrapper_t::qos_level)
      .def_readwrite("fm_mem_region",
                     &aipu_create_job_cfg_wrapper_t::fm_mem_region)
      .def_readwrite("fm_idxes", &aipu_create_job_cfg_wrapper_t::fm_idxes)
      .def_property(
          "dynshape",
          [](aipu_create_job_cfg_wrapper_t &self) {
            py::list shapes;
            for (auto &input : self.dynshape.shape_items) {
              py::list shape;
              for (auto &dim : input)
                shape.append(dim);
              shapes.append(shape);
              /* py::list shape(input.begin(), input.end()); */
            }
            return shapes;
          },
          [](aipu_create_job_cfg_wrapper_t &self, py::list &l) {
            std::vector<std::vector<uint32_t>> shapes;
            for (auto &i : l) {
              if (py::isinstance<py::list>(i)) {
                std::vector<uint32_t> shape;
                for (auto &j : i) {
                  shape.push_back(py::cast<uint32_t>(j));
                }
                shapes.push_back(shape);
              } else
                fprintf(stderr, "please provide list nest for dynamic input "
                                "shape, even if there is only 1 input");
            }
            self.dynshape.shape_items = shapes;
          });

  py::enum_<aipu_job_part_t>(m, "aipu_job_part_t")
      .value("AIPU_JOB_PART0", aipu_job_part_t::AIPU_JOB_PART0)
      .value("AIPU_JOB_PART1", aipu_job_part_t::AIPU_JOB_PART1)
      .value("AIPU_JOB_PART2", aipu_job_part_t::AIPU_JOB_PART2)
      .value("AIPU_JOB_PART3", aipu_job_part_t::AIPU_JOB_PART3)
      .export_values();

  py::enum_<aipu_job_qos_t>(m, "aipu_job_qos_t")
      .value("AIPU_JOB_QOS_SLOW", aipu_job_qos_t::AIPU_JOB_QOS_SLOW)
      .value("AIPU_JOB_QOS_HIGH", aipu_job_qos_t::AIPU_JOB_QOS_HIGH)
      .export_values();

  py::enum_<aipu_mem_region_t>(m, "aipu_mem_region_t")
      .value("AIPU_MEM_REGION_DEFAULT",
             aipu_mem_region_t::AIPU_MEM_REGION_DEFAULT)
      .value("AIPU_MEM_REGION_SRAM", aipu_mem_region_t::AIPU_MEM_REGION_SRAM)
      .value("AIPU_MEM_REGION_DTCM", aipu_mem_region_t::AIPU_MEM_REGION_DTCM)
      .export_values();

  py::enum_<aipu_ioctl_cmd_t>(m, "aipu_ioctl_cmd_t")
      .value("AIPU_IOCTL_SET_PROFILE", aipu_ioctl_cmd_t::AIPU_IOCTL_SET_PROFILE)
      .value("AIPU_IOCTL_GET_AIPUBIN_BUILDVERSION",
             aipu_ioctl_cmd_t::AIPU_IOCTL_GET_AIPUBIN_BUILDVERSION)
      .value("AIPU_IOCTL_GET_DS_RANK", aipu_ioctl_cmd_t::AIPU_IOCTL_GET_DS_RANK)
      .value("AIPU_IOCTL_GET_DS_DIM_CONSTRAINT",
             aipu_ioctl_cmd_t::AIPU_IOCTL_GET_DS_DIM_CONSTRAINT)
      .value("AIPU_IOCTL_SET_DYNAMIC_ASID1",
             aipu_ioctl_cmd_t::AIPU_IOCTL_SET_DYNAMIC_ASID1)
      .value("AIPU_IOCTL_ALLOC_SHARE_BUF",
             aipu_ioctl_cmd_t::AIPU_IOCTL_ALLOC_SHARE_BUF)
      .value("AIPU_IOCTL_FREE_SHARE_BUF",
             aipu_ioctl_cmd_t::AIPU_IOCTL_FREE_SHARE_BUF)
      .value("AIPU_IOCTL_ALLOC_DMABUF",
             aipu_ioctl_cmd_t::AIPU_IOCTL_ALLOC_DMABUF)
      .value("AIPU_IOCTL_FREE_DMABUF", aipu_ioctl_cmd_t::AIPU_IOCTL_FREE_DMABUF)
      /* python side use mmap to access dma-buf directly */
      // .value("AIPU_IOCTL_WRITE_DMABUF",
      // aipu_ioctl_cmd_t::AIPU_IOCTL_WRITE_DMABUF)
      // .value("AIPU_IOCTL_READ_DMABUF",
      // aipu_ioctl_cmd_t::AIPU_IOCTL_READ_DMABUF)
      .value("AIPU_IOCTL_GET_DMABUF_INFO",
             aipu_ioctl_cmd_t::AIPU_IOCTL_GET_DMABUF_INFO)
      .value("AIPU_IOCTL_ATTACH_DMABUF",
             aipu_ioctl_cmd_t::AIPU_IOCTL_ATTACH_DMABUF)
      .value("AIPU_IOCTL_DETACH_DMABUF",
             aipu_ioctl_cmd_t::AIPU_IOCTL_DETACH_DMABUF)
      .value("AIPU_IOCTL_GET_VERSION", aipu_ioctl_cmd_t::AIPU_IOCTL_GET_VERSION)
      .value("AIPU_IOCTL_ABORT_CMDPOOL",
             aipu_ioctl_cmd_t::AIPU_IOCTL_ABORT_CMDPOOL)
      .value("AIPU_IOCTL_ENABLE_TICKCOUNTER",
             aipu_ioctl_cmd_t::AIPU_IOCTL_ENABLE_TICKCOUNTER)
      .value("AIPU_IOCTL_DISABLE_TICKCOUNTER",
             aipu_ioctl_cmd_t::AIPU_IOCTL_DISABLE_TICKCOUNTER)
      .export_values();

  py::enum_<aipu_share_case_type_t>(m, "aipu_share_case_type_t")
      .value("AIPU_SHARE_BUF_IN_ONE_PROCESS",
             aipu_share_case_type_t::AIPU_SHARE_BUF_IN_ONE_PROCESS)
      .value("AIPU_SHARE_BUF_DMABUF",
             aipu_share_case_type_t::AIPU_SHARE_BUF_DMABUF)
      .value("AIPU_SHARE_BUF_CUSTOMED",
             aipu_share_case_type_t::AIPU_SHARE_BUF_CUSTOMED)
      .value("AIPU_SHARE_BUF_ATTACH_DMABUF",
             aipu_share_case_type_t::AIPU_SHARE_BUF_ATTACH_DMABUF)
      .export_values();

  py::enum_<aipu_status_t>(m, "aipu_status_t")
      .value("AIPU_STATUS_SUCCESS", aipu_status_t::AIPU_STATUS_SUCCESS)
      .value("AIPU_STATUS_ERROR_NULL_PTR",
             aipu_status_t::AIPU_STATUS_ERROR_NULL_PTR)
      .value("AIPU_STATUS_ERROR_INVALID_CTX",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_CTX)
      .value("AIPU_STATUS_ERROR_OPEN_DEV_FAIL",
             aipu_status_t::AIPU_STATUS_ERROR_OPEN_DEV_FAIL)
      .value("AIPU_STATUS_ERROR_DEV_ABNORMAL",
             aipu_status_t::AIPU_STATUS_ERROR_DEV_ABNORMAL)
      .value("AIPU_STATUS_ERROR_DEINIT_FAIL",
             aipu_status_t::AIPU_STATUS_ERROR_DEINIT_FAIL)
      .value("AIPU_STATUS_ERROR_INVALID_CONFIG",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_CONFIG)
      .value("AIPU_STATUS_ERROR_UNKNOWN_BIN",
             aipu_status_t::AIPU_STATUS_ERROR_UNKNOWN_BIN)
      .value("AIPU_STATUS_ERROR_GVERSION_UNSUPPORTED",
             aipu_status_t::AIPU_STATUS_ERROR_GVERSION_UNSUPPORTED)
      .value("AIPU_STATUS_ERROR_INVALID_GBIN",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_GBIN)
      .value("AIPU_STATUS_ERROR_TARGET_NOT_FOUND",
             aipu_status_t::AIPU_STATUS_ERROR_TARGET_NOT_FOUND)
      .value("AIPU_STATUS_ERROR_INVALID_GRAPH_ID",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_GRAPH_ID)
      .value("AIPU_STATUS_ERROR_OPEN_FILE_FAIL",
             aipu_status_t::AIPU_STATUS_ERROR_OPEN_FILE_FAIL)
      .value("AIPU_STATUS_ERROR_MAP_FILE_FAIL",
             aipu_status_t::AIPU_STATUS_ERROR_MAP_FILE_FAIL)
      .value("AIPU_STATUS_ERROR_READ_FILE_FAIL",
             aipu_status_t::AIPU_STATUS_ERROR_READ_FILE_FAIL)
      .value("AIPU_STATUS_ERROR_WRITE_FILE_FAIL",
             aipu_status_t::AIPU_STATUS_ERROR_WRITE_FILE_FAIL)
      .value("AIPU_STATUS_ERROR_INVALID_JOB_ID",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_JOB_ID)
      .value("AIPU_STATUS_ERROR_JOB_EXCEPTION",
             aipu_status_t::AIPU_STATUS_ERROR_JOB_EXCEPTION)
      .value("AIPU_STATUS_ERROR_JOB_TIMEOUT",
             aipu_status_t::AIPU_STATUS_ERROR_JOB_TIMEOUT)
      .value("AIPU_STATUS_ERROR_OP_NOT_SUPPORTED",
             aipu_status_t::AIPU_STATUS_ERROR_OP_NOT_SUPPORTED)
      .value("AIPU_STATUS_ERROR_INVALID_OP",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_OP)
      .value("AIPU_STATUS_ERROR_INVALID_SIZE",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_SIZE)
      .value("AIPU_STATUS_ERROR_BUF_ALLOC_FAIL",
             aipu_status_t::AIPU_STATUS_ERROR_BUF_ALLOC_FAIL)
      .value("AIPU_STATUS_ERROR_BUF_FREE_FAIL",
             aipu_status_t::AIPU_STATUS_ERROR_BUF_FREE_FAIL)
      .value("AIPU_STATUS_ERROR_INVALID_CORE_ID",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_CORE_ID)
      .value("AIPU_STATUS_ERROR_RESERVE_SRAM_FAIL",
             aipu_status_t::AIPU_STATUS_ERROR_RESERVE_SRAM_FAIL)
      .value("AIPU_STATUS_ERROR_INVALID_TENSOR_ID",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_TENSOR_ID)
      .value("AIPU_STATUS_ERROR_INVALID_CLUSTER_ID",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_CLUSTER_ID)
      .value("AIPU_STATUS_ERROR_INVALID_PARTITION_ID",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_PARTITION_ID)
      .value("AIPU_STATUS_ERROR_PRINTF_FAIL",
             aipu_status_t::AIPU_STATUS_ERROR_PRINTF_FAIL)
      .value("AIPU_STATUS_ERROR_INVALID_TENSOR_TYPE",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_TENSOR_TYPE)
      .value("AIPU_STATUS_ERROR_INVALID_GM",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_GM)
      .value("AIPU_STATUS_ERROR_INVALID_SEGMMU",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_SEGMMU)
      .value("AIPU_STATUS_ERROR_INVALID_QOS",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_QOS)
      .value("AIPU_STATUS_ERROR_INVALID_TENSOR_CNT",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_TENSOR_CNT)
      .value("AIPU_STATUS_ERROR_TIMEOUT",
             aipu_status_t::AIPU_STATUS_ERROR_TIMEOUT)
      .value("AIPU_STATUS_ERROR_NO_BATCH_QUEUE",
             aipu_status_t::AIPU_STATUS_ERROR_NO_BATCH_QUEUE)
      .value("AIPU_STATUS_ERROR_MARK_SHARED_TENSOR",
             aipu_status_t::AIPU_STATUS_ERROR_MARK_SHARED_TENSOR)
      .value("AIPU_STATUS_ERROR_SET_SHARED_TENSOR",
             aipu_status_t::AIPU_STATUS_ERROR_SET_SHARED_TENSOR)
      .value("AIPU_STATUS_ERROR_DMABUF_SHARED_IO",
             aipu_status_t::AIPU_STATUS_ERROR_DMABUF_SHARED_IO)
      .value("AIPU_STATUS_ERROR_GET_SHAPE_FAILED",
             aipu_status_t::AIPU_STATUS_ERROR_GET_SHAPE_FAILED)
      .value("AIPU_STATUS_ERROR_SET_SHAPE_FAILED",
             aipu_status_t::AIPU_STATUS_ERROR_SET_SHAPE_FAILED)
      .value("AIPU_STATUS_ERROR_NOT_CONFIG_SHAPE",
             aipu_status_t::AIPU_STATUS_ERROR_NOT_CONFIG_SHAPE)
      .value("AIPU_STATUS_ERROR_UNMATCH_OUT_SHAPE",
             aipu_status_t::AIPU_STATUS_ERROR_UNMATCH_OUT_SHAPE)
      .value("AIPU_STATUS_ERROR_ZERO_TENSOR_SIZE",
             aipu_status_t::AIPU_STATUS_ERROR_ZERO_TENSOR_SIZE)
      .value("AIPU_STATUS_ERROR_ALLOC_GRIP_ID",
             aipu_status_t::AIPU_STATUS_ERROR_ALLOC_GRIP_ID)
      .value("AIPU_STATUS_ERROR_ALLOC_GROUP_ID",
             aipu_status_t::AIPU_STATUS_ERROR_ALLOC_GROUP_ID)
      .value("AIPU_STATUS_ERROR_NOT_FOUND_IN_HASHTABLE",
             aipu_status_t::AIPU_STATUS_ERROR_NOT_FOUND_IN_HASHTABLE)
      .value("AIPU_STATUS_ERROR_INVALID_COREDUMP",
             aipu_status_t::AIPU_STATUS_ERROR_INVALID_COREDUMP)
      .value("AIPU_STATUS_ERROR_JOB_DISPATCH_FAIL",
             aipu_status_t::AIPU_STATUS_ERROR_JOB_DISPATCH_FAIL)
      .value("AIPU_STATUS_MAX", aipu_status_t::AIPU_STATUS_MAX)
      .value("AIPU_STATUS_ERROR_UNKNOWN_ERROR",
             aipu_status_t::AIPU_STATUS_ERROR_UNKNOWN_ERROR)
      .value("AIPU_STATUS_ERROR_KEYBOARD_INTERRUPT",
             aipu_status_t::AIPU_STATUS_ERROR_KEYBOARD_INTERRUPT)
      .value("AIPU_STATUS_ERROR_SYSTEM_ERR",
             aipu_status_t::AIPU_STATUS_ERROR_SYSTEM_ERR)
      .value("AIPU_STATUS_ERROR_OUT_OF_SPEC",
             aipu_status_t::AIPU_STATUS_ERROR_OUT_OF_SPEC)
      .value("AIPU_STATUS_ERROR_OUT_OF_AIFF_SPEC",
             aipu_status_t::AIPU_STATUS_ERROR_OUT_OF_AIFF_SPEC)
      .value("AIPU_STATUS_ERROR_OUT_OF_TPC_SPEC",
             aipu_status_t::AIPU_STATUS_ERROR_OUT_OF_TPC_SPEC)
      .value("AIPU_STATUS_ERROR_OUT_OF_DMA_SPEC",
             aipu_status_t::AIPU_STATUS_ERROR_OUT_OF_DMA_SPEC)
      .value("AIPU_STATUS_ERROR_OUT_OF_MEM_RANGE",
             aipu_status_t::AIPU_STATUS_ERROR_OUT_OF_MEM_RANGE)
      .value("AIPU_STATUS_ERROR_OUT_OF_SRAM_RANGE",
             aipu_status_t::AIPU_STATUS_ERROR_OUT_OF_SRAM_RANGE)
      .value("AIPU_STATUS_ERROR_OUT_OF_LSRAM0_RANGE",
             aipu_status_t::AIPU_STATUS_ERROR_OUT_OF_LSRAM0_RANGE)
      .value("AIPU_STATUS_ERROR_OUT_OF_LSRAM1_RANGE",
             aipu_status_t::AIPU_STATUS_ERROR_OUT_OF_LSRAM1_RANGE)
      .value("AIPU_STATUS_ERROR_OUT_OF_GSRAM0_RANGE",
             aipu_status_t::AIPU_STATUS_ERROR_OUT_OF_GSRAM0_RANGE)
      .value("AIPU_STATUS_ERROR_OUT_OF_GSRAM1_RANGE",
             aipu_status_t::AIPU_STATUS_ERROR_OUT_OF_GSRAM1_RANGE)
      .value("AIPU_STATUS_ERROR_ARITHMETIC_ERR",
             aipu_status_t::AIPU_STATUS_ERROR_ARITHMETIC_ERR)
      .value("AIPU_STATUS_ERROR_FLOAT_POINT_ERR",
             aipu_status_t::AIPU_STATUS_ERROR_FLOAT_POINT_ERR)
      .value("AIPU_STATUS_ERROR_UNDERFLOW_ERR",
             aipu_status_t::AIPU_STATUS_ERROR_UNDERFLOW_ERR)
      .value("AIPU_STATUS_ERROR_OVERFLOW_ERR",
             aipu_status_t::AIPU_STATUS_ERROR_OVERFLOW_ERR)
      .value("AIPU_STATUS_ERROR_NOT_A_NUMBER_ERR",
             aipu_status_t::AIPU_STATUS_ERROR_NOT_A_NUMBER_ERR)
      .value("AIPU_STATUS_ERROR_INFINITY_ERR",
             aipu_status_t::AIPU_STATUS_ERROR_INFINITY_ERR)
      .value("AIPU_STATUS_ERROR_STRING_LENGTH_ERR",
             aipu_status_t::AIPU_STATUS_ERROR_STRING_LENGTH_ERR)
      .value("AIPU_STATUS_ERROR_ZERO_DIVISION_ERR",
             aipu_status_t::AIPU_STATUS_ERROR_ZERO_DIVISION_ERR)
      .export_values();

  py::class_<NPU>(m, "NPU")
      .def(py::init())

      .def("aipu_init_context", &NPU::aipu_init_context_py)

      .def("aipu_deinit_context", &NPU::aipu_deinit_context_py)

      .def("aipu_get_error_message", &NPU::aipu_get_error_message_py,
           py::arg("status"), py::return_value_policy::copy)

      .def("aipu_config_global", &NPU::aipu_config_global_py, py::arg("types"),
           py::arg("cfg") = py::none())

      .def("aipu_load_graph_from_file", &NPU::aipu_load_graph_py,
           py::arg("graph_bin"), py::arg("cfg") = py::none(),
           py::return_value_policy::copy)

      .def("aipu_load_graph", &NPU::aipu_load_graph_helper_py,
           py::arg("graph_buffer"), py::arg("graph_size"),
           py::arg("cfg") = py::none(), py::return_value_policy::copy)

      .def("aipu_load_share_weight_graph",
           &NPU::aipu_load_share_weight_graph_py, py::arg("graph_bin"),
           py::arg("cfg") = py::none(), py::return_value_policy::copy)

      .def("aipu_unload_graph", &NPU::aipu_unload_graph_py, py::arg("graph_id"))

      .def("aipu_create_job", &NPU::aipu_create_job_py, py::arg("graph_id"),
           py::arg("cfg") = py::none())

      .def("aipu_config_job", &NPU::aipu_config_job_py, py::arg("job_id"),
           py::arg("types"), py::arg("cfg"))

      .def("aipu_finish_job", &NPU::aipu_finish_job_py, py::arg("job_id"),
           py::arg("time_out") = -1)

      .def("aipu_flush_job", &NPU::aipu_flush_job_py, py::arg("job_id"),
           py::arg("py_cb") = nullptr)

      .def("aipu_get_job_status", &NPU::aipu_get_job_status_py,
           py::arg("job_id"), py::arg("timeout") = -1)

      .def("aipu_clean_job", &NPU::aipu_clean_job_py, py::arg("job_id"))

      .def("aipu_get_tensor_count", &NPU::aipu_get_tensor_count_py,
           py::arg("graph_id"), py::arg("type"), py::return_value_policy::copy)

      .def("aipu_get_tensor_descriptor", &NPU::aipu_get_tensor_descriptor_py,
           py::arg("graph_id"), py::arg("type"), py::arg("tensor"),
           py::return_value_policy::copy)

      .def("aipu_load_tensor_from_file", &NPU::aipu_load_tensor_file_py,
           py::arg("job_id"), py::arg("tensor"), py::arg("filename"))

      .def("aipu_load_tensor",
           py::overload_cast<uint64_t, uint32_t, py::handle>(
               &NPU::aipu_load_tensor_py),
           py::arg("job_id"), py::arg("tensor"), py::arg("hdl"))

      .def("aipu_get_tensor", &NPU::aipu_get_tensor_py, py::arg("job_id"),
           py::arg("type"), py::arg("tensor"), py::return_value_policy::copy)

      .def("aipu_ioctl", &NPU::aipu_ioctl_py, py::arg("cmd"),
           /* py::arg("kwargs"),*/
           py::return_value_policy::copy)

      .def("aipu_specify_iobuf", &NPU::aipu_specify_iobuf_py, py::arg("job_id"),
           py::arg("cfg") = py::none())

      .def("aipu_create_batch_queue", &NPU::aipu_create_batch_queue_py,
           py::arg("graph_id"), py::return_value_policy::copy)

      .def("aipu_clean_batch_queue", &NPU::aipu_clean_batch_queue_py,
           py::arg("graph_id"), py::arg("queue_id"),
           py::return_value_policy::copy)

      .def("aipu_config_batch_dump", &NPU::aipu_config_batch_dump_py,
           py::arg("graph_id"), py::arg("queue_id"), py::arg("types"),
           py::arg("dump_cfg") = std::map<std::string, std::string>{},
           py::return_value_policy::copy)

      .def(
          "aipu_add_batch",
          py::overload_cast<uint64_t, uint32_t, std::vector<py::handle> &,
                            std::vector<py::handle> &>(&NPU::aipu_add_batch_py),
          py::arg("graph_id"), py::arg("queue_id"), py::arg("inputs"),
          py::arg("outputs"), py::return_value_policy::copy)

      .def("aipu_finish_batch", &NPU::aipu_finish_batch_py, py::arg("graph_id"),
           py::arg("queue_id"), py::arg("cfg") = py::none());
}